# import chainlit as cl
# import asyncio
# from autogen_agentchat.agents import AssistantAgent
# from autogen_agentchat.teams import RoundRobinGroupChat
# from autogen_agentchat.conditions import TextMentionTermination
# from autogen_ext.models.openai import OpenAIChatCompletionClient
# from dotenv import load_dotenv
# import os
# load_dotenv()
# api_keys = os.getenv("API_KEY")

# os.environ["SSL_CERT_FILE"] = ""  # ç¦ç”¨SSLè¯ä¹¦éªŒè¯

# # åˆå§‹åŒ–Chainlitç•Œé¢
# @cl.on_chat_start
# async def init_agents():
#     await cl.Message(content="æ‚¨å¥½ï¼Œè¿™é‡Œæ˜¯äº§å“è®¾è®¡AIå›¢é˜Ÿï¼ŒåŒ…å«äº§å“ç»ç†ã€å¼€å‘å·¥ç¨‹å¸ˆå’Œæµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·è¾“å…¥æ‚¨çš„éœ€æ±‚").send()

# # åˆ›å»ºä¸‰ä¸ªä¸“ä¸šAgent
# async def create_agents():
#     model_client = OpenAIChatCompletionClient(
#         model="glm-4-plus",
#         api_key=api_keys,
#         base_url="https://open.bigmodel.cn/api/paas/v4/",
#         model_info={
#             "vision": False,
#             "function_calling": True,
#             "json_output": True,
#             "family": "glm",
#         },
        
#     )

#     # äº§å“ç»ç†Agent
#     pm_agent = AssistantAgent(
#         name="Product_Manager",
#         model_client=model_client,
#         system_message="ä½ æ˜¯æœ‰åå¹´ç»éªŒçš„äº§å“ç»ç†ï¼Œæ“…é•¿éœ€æ±‚åˆ†æå’ŒåŠŸèƒ½è®¾è®¡ã€‚è¯·å…ˆç¡®è®¤ç”¨æˆ·éœ€æ±‚ï¼Œç„¶åæå‡ºäº§å“æ–¹æ¡ˆã€‚"
#     )

#     # å¼€å‘å·¥ç¨‹å¸ˆAgent 
#     dev_agent = AssistantAgent(
#         name="Develop_Engineer",
#         model_client=model_client,
#         system_message="ä½ æ˜¯å…¨æ ˆå¼€å‘ä¸“å®¶ï¼Œæ ¹æ®äº§å“æ–¹æ¡ˆç»™å‡ºæŠ€æœ¯å®ç°æ–¹æ¡ˆã€‚éœ€è¦è¯„ä¼°å®ç°éš¾åº¦ï¼Œæå‡ºæŠ€æœ¯é£é™©ã€‚"
#     )

#     # æµ‹è¯•å·¥ç¨‹å¸ˆAgent
#     test_agent = AssistantAgent(
#         name="QA_Engineer",
#         model_client=model_client,
#         system_message="ä½ æ˜¯èµ„æ·±QAå·¥ç¨‹å¸ˆï¼Œé’ˆå¯¹æŠ€æœ¯æ–¹æ¡ˆæå‡ºæµ‹è¯•è¦ç‚¹å’Œæ½œåœ¨é—®é¢˜ã€‚æœ€ç»ˆæ¶ˆæ¯å¿…é¡»ä»¥TERMINATEç»“å°¾ï¼"
#     )

#     return [pm_agent, dev_agent, test_agent]

# # è¿è¡ŒAIå›¢é˜Ÿåä½œ
# async def run_team(task: str):
#     agents = await create_agents()
    
#     # è®¾ç½®ç»ˆæ­¢æ¡ä»¶ï¼ˆå½“å‡ºç°"æ–¹æ¡ˆé€šè¿‡"æ—¶åœæ­¢ï¼‰
#     termination = TextMentionTermination("TERMINATE")
    
#     # åˆ›å»ºè½®è¯¢å¼ç¾¤èŠ
#     team = RoundRobinGroupChat(
#         participants=agents,
#         termination_condition=termination,
#         max_turns=12  # æœ€å¤šä¸‰è½®è®¨è®º
#     )

#     # æµå¼å¤„ç†å“åº”
#     response_stream = team.run_stream(task=task)
#     async for msg in response_stream:
#         # if msg.source != "user" and msg.content:
#         if hasattr(msg, "source") and msg.content:
#             await cl.Message(content=f"{msg.source}ï¼š{msg.content}").send()

# # Chainlitæ¶ˆæ¯å¤„ç†
# @cl.on_message
# async def main(message: cl.Message):
#     await run_team(message.content)

import chainlit as cl
import asyncio
import httpx
import numpy as np
from pycparser import c_parser, c_ast
from sklearn.metrics import jaccard_score
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_core import CancellationToken
from dotenv import load_dotenv
import os
from langchain_chroma import Chroma
# from langchain_ollama import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI
import re
import json
from inference_userinput import ModelPredictor
from inference_userinput import wrap_code_in_function
from ast_eval_for import ASTComparator
import traceback
from autogen_agentchat.base import TaskResult

os.environ['SSL_CERT_FILE'] = '/etc/ssl/certs/ca-certificates.crt' 


def extract_code(msg: str) -> str:
    """ä»æ¶ˆæ¯ä¸­æå–Cä»£ç å—"""
    try:
        # åŒ¹é…å¸¦è¯­è¨€æ ‡è¯†çš„ä»£ç å—
        code_blocks = re.findall(r'```c\n(.*?)\n```', msg, re.DOTALL)
        if code_blocks:
            return max(code_blocks, key=len).strip()  # é€‰æ‹©æœ€é•¿çš„ä»£ç å—
        
        # åŒ¹é…æ— è¯­è¨€æ ‡è¯†çš„ä»£ç å—
        code_blocks = re.findall(r'```\n(.*?)\n```', msg, re.DOTALL)
        if code_blocks:
            return max(code_blocks, key=len).strip()
            
        # åŒ¹é…ç¼©è¿›ä»£ç å—ï¼ˆ4ç©ºæ ¼æˆ–tabå¼€å¤´ï¼‰
        indented_code = re.findall(r'^(\s{4,}|\t+)(.*?)(?=\n\S|\Z)', msg, re.MULTILINE|re.DOTALL)
        if indented_code:
            return '\n'.join([line[1] for line in indented_code]).strip()
            
    except Exception as e:
        print(f"ä»£ç è§£æå¼‚å¸¸: {str(e)}")
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œè¿”å›åŸå§‹æ¶ˆæ¯ä¸­çš„å‰5è¡Œ
    return '\n'.join(msg.split('\n')[:5]).strip()

def parse_evaluation(msg: str) -> str:
    """è§£æè¯„ä¼°ç»“æœå¹¶æ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€"""
    try:
        # æå–JSONå­—ç¬¦ä¸²ï¼ˆå¤„ç†ä¸¤ç§å¸¸è§æ ¼å¼ï¼‰
        json_str = re.search(r'\{[\s\S]*\}', msg).group()
        json_str = json_str.replace("'", '"')  # è½¬æ¢å•å¼•å·ä¸ºåŒå¼•å·
        
        result = json.loads(json_str)
        
        # æ„å»ºè‡ªç„¶è¯­è¨€è§£é‡Š
        analysis = [
            f"ğŸ›¡ï¸ ä¿çœŸåº¦ï¼š{result['fidelity']*100:.1f}% (åŸå§‹ä»£ç ç»“æ„ä¿ç•™ç¨‹åº¦)",
            f"âœ… æ­£ç¡®æ€§ï¼š{result['correctness']*100:.1f}% (ç¬¦åˆOpenMPæœ€ä½³å®è·µ)",
            f"ğŸ“Š ç»¼åˆç½®ä¿¡åº¦ï¼š{result['confidence']*100:.1f}%"
        ]
        
        # æ·»åŠ è¯„ä¼°å»ºè®®
        if result['confidence'] >= 0.8:
            analysis.append("ğŸŒŸ å»ºè®®ï¼šé«˜ç½®ä¿¡åº¦ç»“æœï¼Œå¯ç›´æ¥éƒ¨ç½²")
        elif result['confidence'] >= 0.6:
            analysis.append("ğŸ’¡ å»ºè®®ï¼šä¸­ç­‰ç½®ä¿¡åº¦ï¼Œå»ºè®®äººå·¥å¤æ ¸")
        else:
            analysis.append("âš ï¸ å»ºè®®ï¼šä½ç½®ä¿¡åº¦ï¼Œéœ€è¦ä¸“å®¶ä»‹å…¥")
            
        return '\n\n'.join(analysis)
        
    except Exception as e:
        print(f"è¯„ä¼°è§£æå¼‚å¸¸: {str(e)}")
        # å°è¯•æå–æ•°å€¼ä¿¡æ¯
        numbers = re.findall(r'\d+\.\d+', msg)
        if len(numbers) >=3:
            return f"""
            ğŸ§® æ•°å€¼è¯„ä¼°ï¼ˆè‡ªåŠ¨è§£æï¼‰ï¼š
            - ä¿çœŸåº¦ï¼š{float(numbers[0])*100:.1f}%
            - æ­£ç¡®æ€§ï¼š{float(numbers[1])*100:.1f}%
            - ç½®ä¿¡åº¦ï¼š{float(numbers[2])*100:.1f}%
            """
        return "ğŸ” è¯„ä¼°ç»“æœè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥åŸå§‹æ•°æ®"


# å°†åˆå§‹åŒ–é€»è¾‘ç§»åˆ°Chainlitçš„èŠå¤©å¯åŠ¨å›è°ƒ
@cl.on_chat_start
async def init_chat():
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    api_keys = os.getenv("API_KEY")
    
    # åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
    model_client = OpenAIChatCompletionClient(
        model="glm-4-plus",
        api_key=api_keys,
        base_url="https://open.bigmodel.cn/api/paas/v4/",
        model_info={
            "vision": False,
            "function_calling": True,
            "json_output": True,
            "family": "glm",
        },
    )

    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    # embeddings = OllamaEmbeddings(model="bge-m3:latest")
    embeddings = OpenAIEmbeddings(
    model="bge-m3",  # éœ€ä¸ FastChat éƒ¨ç½²çš„ Embedding æ¨¡å‹åç§°ä¸€è‡´
    openai_api_base="http://0.0.0.0:8200/v1",  # FastChat æœåŠ¡åœ°å€
    openai_api_key="none",  # è‹¥æ— é‰´æƒï¼Œå¯è®¾ä¸ºä»»æ„å€¼
    http_client=httpx.Client(verify=False)
    )
    vector_store = Chroma(
        persist_directory="chroma_db_train_code",
        embedding_function=embeddings,
        collection_name="openmp_code"
    )
    
    # åˆ›å»ºAgentå›¢é˜Ÿï¼ˆæ¯ä¸ªä¼šè¯ç‹¬ç«‹å®ä¾‹ï¼‰
    # termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(50)

    """
    å·¥å…·å‡½æ•°å®šä¹‰ -> RetrievalExpert
    """
    async def retrieve_code_tool(code: str, k: int=3) -> list[str]:
        """æ£€ç´¢ç›¸ä¼¼ä»£ç å—
        Args:
            code: è¾“å…¥çš„ä»£ç æ®µ
            k: è¿”å›çš„ç›¸ä¼¼ä»£ç æ•°é‡
        Returns:
            list[str]: ç›¸ä¼¼ä»£ç åˆ—è¡¨
        """
        # embedding = await ollama_client.create_embedding(model="bge-m3:latest", input=code)
        # å‡è®¾å·²æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­¤å¤„ç”¨éšæœºæ•°æ®æ¨¡æ‹Ÿç›¸ä¼¼ä»£ç å—
        try:
            # ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œç›¸ä¼¼æ€§æœç´¢
            # results = vector_store.similarity_search(
            #     query=code,
            #     k=k
            # )

            results = "for(int i = 0; i < 10; i++) { printf(\"%d\\n\", i);"

            print("result:", results)
            
            # æå–æœç´¢ç»“æœçš„ä»£ç å†…å®¹
            similar_codes = [doc.page_content for doc in results]
            
            return similar_codes
        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {e}")
            return [f"æ£€ç´¢å¤±è´¥: {str(e)}"]
        
    """
        å·¥å…·å‡½æ•°å®šä¹‰ -> CodeGenerator
    """
    def generate_code_tool(code: str) -> str:
        """æ ¹æ®ä»£ç é¢„æµ‹ç»“æœç”Ÿæˆå¸¦OpenMPæŒ‡ä»¤çš„ä»£ç 
        Args:
            code: è¾“å…¥çš„Cä»£ç å­—ç¬¦ä¸²
        Returns:
            str: æ·»åŠ äº†OpenMPæŒ‡ä»¤çš„ä»£ç 
        """
        # åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = ModelPredictor()
        
        # è·å–é¢„æµ‹ç»“æœ
        result = predictor.predict(code)

        # print("result:", result)

        if not result:
            return code  # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œè¿”å›åŸå§‹ä»£ç 
        if not result['pragma']:
            return code  # å¦‚æœé¢„æµ‹ä¸èƒ½å¹¶è¡Œï¼Œè¿”å›åŸå§‹ä»£ç 
        
        # æ„å»ºOpenMPæŒ‡ä»¤
        omp_directive = "#pragma omp parallel for"
        clauses = []
        
        if result['private']:
            clauses.append("private")
                
        if result['reduction']:
            clauses.append("reduction")
        
        # ç»„è£…å®Œæ•´æŒ‡ä»¤
        if clauses:
            omp_directive += " " + " ".join(clauses)
        
        # åˆå§‹åŒ–LLM
        generate_client = OpenAI(
        api_key=api_keys,
        base_url="https://open.bigmodel.cn/api/paas/v4/"
        )

        prompts = f"""
            è¯·æ ¹æ®ä»¥ä¸‹OpenMPæŒ‡ä»¤ä¿®æ”¹ç»™å®šçš„Cè¯­è¨€forå¾ªç¯ä»£ç ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æ‰§è¡Œï¼š
            1. åªå°†æŒ‡ä»¤æ’å…¥åˆ°æœ€åˆé€‚çš„å¾ªç¯ä½ç½®
            2. ä¿æŒåŸå§‹ä»£ç é€»è¾‘ä¸å˜
            3. ä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–è§£é‡Š
            4. ç¡®ä¿ä»£ç æ ¼å¼æ­£ç¡®ï¼Œä¿ç•™åŸå§‹ç¼©è¿›

            åŸå§‹ä»£ç ï¼š
            {code}

            éœ€è¦æ’å…¥çš„OpenMPæŒ‡ä»¤ï¼š
            {omp_directive}

            ä¿®æ”¹åçš„å®Œæ•´ä»£ç ï¼š"""

        completion = generate_client.chat.completions.create(
        model="glm-4-plus", 
        messages=[    
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„OpenMPç¨‹åºå‘˜ï¼Œèƒ½å¤Ÿé€šè¿‡ä»£ç æ¨¡å¼è¯†åˆ«è‡ªåŠ¨æ¨æ–­æ­£ç¡®çš„å¹¶è¡ŒåŒ–å˜é‡ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å°†ç»™å®šçš„OpenMPæŒ‡ä»¤æ’å…¥åˆ°ä»£ç ä¸­ã€‚ä½ åªè¿”å›æœ€ç»ˆçš„ä»£ç ï¼Œä¸åŒ…å«ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Šã€‚"},    
        {"role": "user", "content": prompts}
            ])
        
        # è¿”å›ä¿®æ”¹åçš„ä»£ç 
        openmp_code = completion.choices[0].message.content
        
        return openmp_code


    """
        å·¥å…·å‡½æ•°å®šä¹‰ -> EvaluationJudge
    """
    # å·¥å…·å‡½æ•°å®šä¹‰ (åŸEvaluationJudgeçš„åŠŸèƒ½ - ASTç›¸ä¼¼åº¦è®¡ç®—)
    def ast_similarity_tool(code1: str, code2: str) -> float:
        """è®¡ç®—ASTç»“æ„ç›¸ä¼¼åº¦"""
        comparator = ASTComparator()

        result = comparator.ast_similarity(code1, code2)

        return round(result,3)



    # å·¥å…·å‡½æ•°å®šä¹‰ (åŸCorrectionEngineerçš„åŠŸèƒ½ - ä¿®æ­£å»ºè®®)
    def correct_code_tool(code: str, confidence: float) -> dict:
        """åŸºäºç½®ä¿¡åº¦æä¾›ä»£ç ä¿®æ­£å»ºè®®"""
        if confidence >= 0.8:
            return {"status": "high", "code": code}
        elif confidence >= 0.6:
            # ä½¿ç”¨æ¨¡å‹åˆ†æä»£ç å¹¶ç»™å‡ºå»ºè®®
            analysis_result = model_client.complete(
                messages=[{
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä¸ª OpenMP å¹¶è¡ŒåŒ–ä¸“å®¶ã€‚è¯·åˆ†æç»™å®šçš„ä»£ç ï¼Œ
                    è¯†åˆ«æ½œåœ¨çš„ä¼˜åŒ–æœºä¼šï¼Œå¹¶æä¾›å…·ä½“çš„ OpenMP ä¼˜åŒ–å»ºè®®ã€‚
                    é‡ç‚¹å…³æ³¨ï¼š
                    1. æ•°æ®ä¾èµ–å…³ç³»
                    2. å¹¶è¡ŒåŒ–æ½œåŠ›
                    3. å¯èƒ½çš„æ€§èƒ½ç“¶é¢ˆ
                    4. é€‚ç”¨çš„ OpenMP å­å¥
                    """
                }, {
                    "role": "user",
                    "content": f"è¯·åˆ†æè¿™æ®µä»£ç å¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼š\n{code}"
                }]
            )
            suggestion = analysis_result.choices[0].message.content
            return {
                "status": "medium", 
                "code": code, 
                "suggestion": suggestion
            }
        else:
            return {
                "status": "low", 
                "alert": "éœ€è¦äººå·¥å®¡æ ¸",
                "reason": model_client.complete(
                    messages=[{
                        "role": "user",
                        "content": f"è¯·è§£é‡Šä¸ºä»€ä¹ˆè¿™æ®µä»£ç çš„å¹¶è¡ŒåŒ–ç½®ä¿¡åº¦è¾ƒä½ï¼š\n{code}"
                    }]
                ).choices[0].message.content
            }
    
    coordinator = AssistantAgent(
    name="Coordinator",
    description="æµç¨‹æ§åˆ¶åè°ƒå‘˜",
    model_client=model_client,
    system_message="""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä»»åŠ¡æµç¨‹æ§åˆ¶Agentï¼Œå¿…é¡»æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š
    1. å½“ç”¨æˆ·æäº¤ä»£ç åï¼Œç«‹å³è®©CodeAgentç”ŸæˆOpenMPä»£ç 
    2. è·å–ç”Ÿæˆçš„OpenMPä»£ç åï¼Œè¦æ±‚RetrievalAgentæ£€ç´¢ç›¸ä¼¼ä»£ç 
    3. è·å¾—ç›¸ä¼¼ä»£ç åï¼Œè¦æ±‚EvaluationAgentåšä»¥ä¸‹è¯„ä¼°ï¼š
    a) è®¡ç®—ç”Ÿæˆä»£ç ä¸ç”¨æˆ·åŸå§‹ä»£ç çš„ASTç›¸ä¼¼åº¦ï¼ˆä¿ç•™åº¦ï¼‰
    b) è®¡ç®—ç”Ÿæˆä»£ç ä¸æ£€ç´¢ç»“æœçš„ASTç›¸ä¼¼åº¦ï¼ˆæ­£ç¡®æ€§ï¼‰
    c) ç»¼åˆè®¡ç®—ç½®ä¿¡åº¦ï¼ˆä¿ç•™åº¦*0.4 + æ­£ç¡®æ€§*0.6ï¼‰
    4. å°†ç½®ä¿¡åº¦ä¼ é€’ç»™CorrectionAgentåšæœ€ç»ˆå†³ç­–

    å¿…é¡»ç¡®ä¿æ­¥éª¤é¡ºåºä¸¥æ ¼æ‰§è¡Œï¼Œæ¯ä¸ªæ­¥éª¤å®Œæˆåå†è¿›è¡Œä¸‹ä¸€æ­¥ã€‚å½“CorrectionAgentè¾“å‡ºæœ€ç»ˆç»“æœåï¼Œå¿…é¡»é™„åŠ TERMINATE""")

    # å¼ºåŒ–CodeAgentçš„ç”Ÿæˆè¦æ±‚
    code_agent = AssistantAgent(
        name="CodeAgent",
        description="OpenMPä»£ç ç”Ÿæˆä¸“å®¶",
        model_client=model_client,
        system_message="""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„OpenMPå¹¶è¡ŒåŒ–ä»£ç ç”Ÿæˆå™¨ï¼Œå¿…é¡»ï¼š
        1. ä¸¥æ ¼ä½¿ç”¨generate_code_toolç”Ÿæˆä»£ç 
        2. ç¡®ä¿ç”Ÿæˆçš„ä»£ç ç¬¦åˆä»¥ä¸‹è¦æ±‚ï¼š
        - OpenMPæŒ‡ä»¤æ’å…¥ä½ç½®å‡†ç¡®
        - ä¿ç•™åŸå§‹ä»£ç é€»è¾‘ä¸å˜
        - æ­£ç¡®æ·»åŠ private/reductionç­‰å­å¥
        3. ç”Ÿæˆå¤±è´¥æ—¶ä¿ç•™åŸå§‹ä»£ç """,
        tools=[generate_code_tool])

    retrieval_agent = AssistantAgent(
        name="RetrievalAgent",
        description="ä»£ç æ¨¡å¼åŒ¹é…ä¸“å®¶",
        model_client=model_client,
        system_message="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ¨¡å¼è¯†åˆ«Agentï¼Œå¿…é¡»ï¼š
    1. ä½¿ç”¨ retrieve_code_tool æ£€ç´¢ä¸CodeAgentç”Ÿæˆçš„ä»£ç ç»“æ„æœ€ç›¸ä¼¼çš„OpenMPä»£ç 
    2. å…³æ³¨ä»¥ä¸‹åŒ¹é…ç»´åº¦ï¼š
    - å¾ªç¯ç»“æ„ï¼ˆfor/while/do-whileï¼‰
    - æ•°ç»„è®¿é—®æ¨¡å¼
    - å˜é‡ä¾èµ–å…³ç³»
    - æ•°æ®å¹¶è¡Œç‰¹å¾
    3. è¿”å›å‰3ä¸ªæœ€ç›¸å…³çš„ä»£ç ç‰‡æ®µ
    4. è‹¥æ— å¯åŒ¹é…ä»£ç ï¼Œè¿”å›ç©ºåˆ—è¡¨""",
        tools=[retrieve_code_tool]
    )

    # ä¼˜åŒ–è¯„ä¼°é€»è¾‘
    evaluation_agent = AssistantAgent(
        name="EvaluationAgent",
        description="å¤šç»´è¯„ä¼°ä¸“å®¶",
        model_client=model_client,
        system_message="""ä½ è´Ÿè´£æ‰§è¡Œä¸‰ç»´è¯„ä¼°ï¼š
        1. ä¿çœŸåº¦ï¼šç”Ÿæˆä»£ç  vs åŸå§‹ä»£ç çš„ASTç›¸ä¼¼åº¦ï¼ˆä¿ç•™åŸå§‹é€»è¾‘ï¼‰
        2. æ­£ç¡®æ€§ï¼šç”Ÿæˆä»£ç  vs ç›¸ä¼¼ä»£ç çš„ASTç›¸ä¼¼åº¦ï¼ˆç¬¦åˆæœ€ä½³å®è·µï¼‰
        3. ç½®ä¿¡åº¦ = ä¿çœŸåº¦*0.4 + æ­£ç¡®æ€§*0.6

        å¿…é¡»ä½¿ç”¨ast_similarity_toolè¿›è¡Œè®¡ç®—ï¼Œå¹¶è¿”å›æ ¼å¼ï¼š
        {
            "fidelity": 0.85,
            "correctness": 0.92,
            "confidence": 0.89
        }""",
        tools=[ast_similarity_tool]
    )

    # å¢å¼ºä¿®æ­£Agentçš„å†³ç­–é€»è¾‘
    correction_agent = AssistantAgent(
        name="CorrectionAgent",
        description="å†³ç­–ä»²è£ä¸“å®¶", 
        model_client=model_client,
        system_message="""ä½ æ ¹æ®ç½®ä¿¡åº¦æ‰§è¡Œä¸¥æ ¼å†³ç­–ï¼š
        - â‰¥0.8ï¼šç›´æ¥è¾“å‡ºç”Ÿæˆä»£ç ï¼Œæ ‡æ³¨"é«˜ç½®ä¿¡åº¦å¹¶è¡ŒåŒ–æˆåŠŸ"
        - 0.6~0.8ï¼šåŒæ—¶è¿”å›ç”Ÿæˆä»£ç å’Œä¼˜åŒ–å»ºè®®
        - <0.6ï¼šè¿”å›åŸå§‹ä»£ç å¹¶æ ‡æ³¨"ä½ç½®ä¿¡åº¦ï¼Œå»ºè®®æ‰‹åŠ¨ä¼˜åŒ–"

        å¿…é¡»ä½¿ç”¨correct_code_toolå¤„ç†ï¼Œæœ€ç»ˆæ¶ˆæ¯å¿…é¡»ä»¥TERMINATEç»“å°¾ï¼""",
        tools=[correct_code_tool]
    )
    
    # æ¶ˆæ¯ç»ˆæ­¢æ¡ä»¶
    text_mention_termination = TextMentionTermination("TERMINATE")
    max_messages_termination = MaxMessageTermination(max_messages=50) # å¢åŠ æœ€å¤§æ¶ˆæ¯è½®æ•°
    termination = text_mention_termination | max_messages_termination


    # Agenté€‰æ‹©å™¨æç¤ºè¯ (å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´)
    selector_prompt = """ä¸¥æ ¼æŒ‰ä»¥ä¸‹é€»è¾‘é€‰æ‹©ä¸‹ä¸€ä¸ªAgentï¼š
    1. ç”¨æˆ·è¾“å…¥ => Coordinator
    2. Coordinator => CodeAgent
    3. CodeAgentç”Ÿæˆå => RetrievalAgent 
    4. æ£€ç´¢å®Œæˆå => EvaluationAgent
    5. è¯„ä¼°å®Œæˆå => CorrectionAgent
    6. æœ€ç»ˆå†³ç­–å => TERMINATE

    å½“å‰çŠ¶æ€ï¼š
    {history}

    å¿…é¡»é€‰æ‹©ä¸‹ä¸€ä¸ªæŒ‡å®šAgentï¼Œç¦æ­¢è·³è¿‡æ­¥éª¤ï¼"""
    

    # åˆ›å»ºAgentå›¢é˜Ÿå®ä¾‹
    agent_team = SelectorGroupChat(
        [coordinator, code_agent, retrieval_agent, evaluation_agent, correction_agent], 
        model_client=model_client, 
        termination_condition=termination,
        selector_prompt=selector_prompt,
        allow_repeated_speaker=False,
    )
    

    # å­˜å‚¨åˆ°ç”¨æˆ·ä¼šè¯ä¸Šä¸‹æ–‡
    cl.user_session.set("agent_team", agent_team)
    cl.user_session.set("vector_store", vector_store)



# å¯ä»¥è¿è¡Œï¼Œä½†æ˜¯æ²¡æœ‰agent cot éšè—
@cl.on_message
async def main(message: cl.Message):
    user_code = message.content
    agent_team = cl.user_session.get("agent_team")
    
    main_msg = cl.Message(content="ğŸš€ å¼€å§‹å¤„ç† OpenMP å¹¶è¡ŒåŒ–æµç¨‹...")
    await main_msg.send()
    
    try:
        task = f"ç”¨æˆ·ä»£ç æ®µ:\n{user_code}\nè¯·åè°ƒå„Agentå®Œæˆç”¨æˆ·ä»£ç çš„OpenMPå¹¶è¡ŒåŒ–ä»£ç ç”Ÿæˆã€‚"
        stream = agent_team.run_stream(task=task)
        
        final_code = None
        final_evaluation = None
        
        async for task_result in stream:
            # è·å–å®é™…æ¶ˆæ¯å†…å®¹
            if hasattr(task_result, 'messages'):
                msg_content = task_result.messages[-1].content
            else:
                msg_content = str(task_result)
            
            # åªåœ¨å†…éƒ¨å¤„ç†ä¸­é—´ç»“æœ
            if "CodeAgent" in msg_content:
                final_code = extract_code(msg_content)
            elif "EvaluationAgent" in msg_content:
                final_evaluation = parse_evaluation(msg_content.content)
        
        # åªåœ¨æœ€åæ˜¾ç¤ºæœ€ç»ˆç»“æœ
        if final_code:
            await cl.Message(
                content=f"**ç”Ÿæˆçš„OpenMPä»£ç **:\n```c\n{final_code}\n```",
                parent_id=main_msg.id
            ).send()
        
        if final_evaluation:
            await cl.Message(
                content=f"**è¯„ä¼°æŠ¥å‘Š**:\n{final_evaluation}",
                parent_id=main_msg.id
            ).send()
                
    except Exception as e:
        await cl.Message(
            content=f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}",
            parent_id=main_msg.id
        ).send()
    finally:
        await main_msg.stream_token("\n\nâœ… æµç¨‹æ‰§è¡Œå®Œæ¯•")
